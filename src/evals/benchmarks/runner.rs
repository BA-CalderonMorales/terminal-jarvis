// Benchmark Runner - Orchestrates Benchmark Execution
//
// This module provides the core execution engine for running benchmarks.
// It coordinates scenario loading, execution, validation, and result export.

use crate::evals::benchmarks::{
    validators::PatternMatchValidator, BenchmarkResult, BenchmarkScenario, ValidationResult,
};
use anyhow::{Context, Result};
use std::path::Path;
use std::time::Instant;

/// Orchestrates benchmark execution end-to-end
pub struct BenchmarkRunner {
    output_dir: Option<String>,
}

impl BenchmarkRunner {
    /// Create a new benchmark runner without output directory
    pub fn new() -> Self {
        Self { output_dir: None }
    }

    /// Create a benchmark runner with JSON export output directory
    pub fn with_output_dir(output_dir: String) -> Self {
        Self {
            output_dir: Some(output_dir),
        }
    }

    /// Execute a benchmark scenario against a tool
    ///
    /// # Phase 2 Implementation
    /// Currently uses mock tool output for testing the framework.
    /// Phase 3 will integrate real tool spawning and execution.
    ///
    /// # Arguments
    /// * `scenario` - The benchmark scenario to execute
    /// * `tool_name` - Name of the tool being benchmarked
    ///
    /// # Returns
    /// * `Result<BenchmarkResult>` - Complete execution result with validation
    pub async fn execute(
        &self,
        scenario: &BenchmarkScenario,
        tool_name: &str,
    ) -> Result<BenchmarkResult> {
        let start = Instant::now();

        // Phase 2: Mock execution (Phase 3 will spawn real tool processes)
        let mock_output = self.generate_mock_output(scenario, tool_name);

        let execution_time_ms = start.elapsed().as_millis() as u64;

        // Run validation using scenario's validation config
        let validation = self.validate_output(scenario, &mock_output)?;

        // Extract results
        let score = validation.score;
        let passed = validation.passed;

        // Create benchmark result
        let result = BenchmarkResult {
            benchmark_id: scenario.metadata.id.clone(),
            tool_name: tool_name.to_string(),
            scenario_version: scenario.metadata.version.clone(),
            execution_timestamp: chrono::Utc::now().to_rfc3339(),
            execution_time_ms,
            passed,
            score,
            output: mock_output,
            validation_details: validation,
        };

        // Export to JSON if output directory specified
        if let Some(ref dir) = self.output_dir {
            let path = Path::new(dir);
            std::fs::create_dir_all(path).context("Failed to create output directory")?;
            result
                .export_json(path)
                .context("Failed to export benchmark result")?;
        }

        Ok(result)
    }

    /// Generate mock tool output for testing (Phase 2 implementation)
    ///
    /// Phase 3 will replace this with actual tool spawning and execution
    fn generate_mock_output(&self, scenario: &BenchmarkScenario, tool_name: &str) -> String {
        // For code-completion scenarios, generate realistic code
        if scenario.metadata.category == "code-completion" {
            format!(
                "// Generated by mock tool: {}\n\
                fn add(a: i32, b: i32) -> i32 {{\n    \
                    return a + b;\n\
                }}",
                tool_name
            )
        } else {
            // Generic mock output for other categories
            format!("Mock output for scenario: {}", scenario.metadata.id)
        }
    }

    /// Validate tool output using scenario's validation configuration
    fn validate_output(
        &self,
        scenario: &BenchmarkScenario,
        output: &str,
    ) -> Result<ValidationResult> {
        match scenario.validation.validation_type.as_str() {
            "pattern-match" => {
                let patterns = scenario.validation.expected_patterns.clone();

                let validator = PatternMatchValidator::new(patterns);
                validator.validate(output)
            }
            other => {
                anyhow::bail!("Unsupported validation type: {}", other)
            }
        }
    }
}

impl Default for BenchmarkRunner {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::evals::benchmarks::{
        PromptConfig, ScenarioMetadata, ScoringConfig, ValidationConfig,
    };

    fn create_test_scenario() -> BenchmarkScenario {
        BenchmarkScenario {
            metadata: ScenarioMetadata {
                id: "test-001".to_string(),
                name: "Test Scenario".to_string(),
                category: "code-completion".to_string(),
                version: "1.0.0".to_string(),
                difficulty: "basic".to_string(),
            },
            prompt: PromptConfig {
                template: "Complete this function: fn add(a: i32, b: i32) -> i32".to_string(),
            },
            validation: ValidationConfig {
                validation_type: "pattern-match".to_string(),
                expected_patterns: vec![r"a \+ b".to_string(), r"return".to_string()],
                rules: vec![],
            },
            scoring: ScoringConfig {
                points_possible: 10,
                pass_threshold: 0.75,
            },
        }
    }

    #[tokio::test]
    async fn test_execute_benchmark_success() {
        let runner = BenchmarkRunner::new();
        let scenario = create_test_scenario();

        let result = runner.execute(&scenario, "mock-tool").await.unwrap();

        assert_eq!(result.benchmark_id, "test-001");
        assert_eq!(result.tool_name, "mock-tool");
        assert_eq!(result.scenario_version, "1.0.0");
        assert!(result.passed, "Benchmark should pass with mock output");
        assert_eq!(result.score, 10.0, "Should achieve perfect score");
    }

    #[tokio::test]
    async fn test_execute_benchmark_validation_details() {
        let runner = BenchmarkRunner::new();
        let scenario = create_test_scenario();

        let result = runner.execute(&scenario, "test-tool").await.unwrap();

        // Verify validation details
        assert_eq!(
            result.validation_details.test_case_results.len(),
            2,
            "Should have 2 pattern test cases"
        );
        assert!(result.validation_details.passed, "Validation should pass");
        assert_eq!(
            result.validation_details.score, 10.0,
            "Validation score should be 10.0"
        );
    }

    #[tokio::test]
    async fn test_execute_with_json_export() {
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();
        let runner =
            BenchmarkRunner::with_output_dir(temp_dir.path().to_string_lossy().to_string());

        let scenario = create_test_scenario();
        let result = runner.execute(&scenario, "test-tool").await.unwrap();

        // Verify JSON was exported
        let json_files: Vec<_> = std::fs::read_dir(temp_dir.path())
            .unwrap()
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("json"))
            .collect();

        assert_eq!(json_files.len(), 1, "Should export exactly one JSON file");
        assert!(result.passed, "Exported result should be passing");

        // Verify JSON content
        let json_path = json_files[0].path();
        let json_content = std::fs::read_to_string(&json_path).unwrap();
        let parsed: serde_json::Value = serde_json::from_str(&json_content).unwrap();

        assert_eq!(parsed["benchmark_id"], "test-001");
        assert_eq!(parsed["tool_name"], "test-tool");
        assert_eq!(parsed["passed"], true);
    }

    #[tokio::test]
    async fn test_execute_creates_output_directory() {
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();
        let nested_path = temp_dir.path().join("nested").join("output");

        let runner = BenchmarkRunner::with_output_dir(nested_path.to_string_lossy().to_string());

        let scenario = create_test_scenario();
        let result = runner.execute(&scenario, "test-tool").await.unwrap();

        assert!(nested_path.exists(), "Should create nested directories");
        assert!(result.passed);
    }

    #[tokio::test]
    async fn test_mock_output_generation() {
        let runner = BenchmarkRunner::new();

        let scenario = create_test_scenario();
        let output = runner.generate_mock_output(&scenario, "test-tool");

        // Verify mock output contains expected patterns
        assert!(output.contains("return"), "Should contain 'return'");
        assert!(output.contains("a + b"), "Should contain 'a + b'");
        assert!(
            output.contains("test-tool"),
            "Should mention tool name in comment"
        );
    }

    #[tokio::test]
    async fn test_unsupported_validation_type() {
        let runner = BenchmarkRunner::new();

        let mut scenario = create_test_scenario();
        scenario.validation.validation_type = "unsupported-type".to_string();

        let result = runner.execute(&scenario, "test-tool").await;

        assert!(
            result.is_err(),
            "Should fail with unsupported validation type"
        );
        assert!(result
            .unwrap_err()
            .to_string()
            .contains("Unsupported validation type"));
    }
}
